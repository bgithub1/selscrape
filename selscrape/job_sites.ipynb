{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedgefund careers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sel_scrape as sc\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from IPython.display import display, HTML\n",
    "import re\n",
    "import pathlib\n",
    "from xml.etree import  cElementTree as ET\n",
    "import pdb\n",
    "import traceback\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_XPATH = {\n",
    "    'title':\"//li[@class='job-data title']\",\n",
    "    'href':\"//li[@class='job-data title']/div/a\",\n",
    "    'locs':\"//li[@class='job-data store_id']\",\n",
    "    'cats':\"//li[@class='job-data parent_category']\",\n",
    "    'posted_date':\"//li[@class='job-data compliment']\",\n",
    "    'read_more':\"//a[@class='black-link read-more']\",\n",
    "    'full_description':\"//div[@data-field='responsibilities']\",\n",
    "    'req':\"//b[contains(text(),'requisition')]/ancestor::p[1]/following-sibling::p\"\n",
    "}\n",
    "MAIN_URL =\"https://careers.blackrock.com/job-search-results/\"\n",
    "HOME_FOLDER = str(pathlib.Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = sc.SelScrape(headless=False)\n",
    "sda = sc.SelDictAccess(main_url=MAIN_URL,dict_xpath=DICT_XPATH,sac=scc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "for i in range(1,100):\n",
    "    print(f'procesing page {i}')\n",
    "    try:\n",
    "        sda.sac.goto(f\"https://careers.blackrock.com/job-search-results?pg={i}\")\n",
    "        test_element = sda.find_xpath('title')\n",
    "        if test_element['status'] is not None:\n",
    "            break\n",
    "        d = {\n",
    "            'title':[e.text for e in sda.find_xpath('title')['value']],\n",
    "            'job_url':[e.get_attribute('href') for e in sda.find_xpath('href')['value']],\n",
    "            'location':[e.text for e in sda.find_xpath('locs')['value']],\n",
    "            'cat':[e.text for e in sda.find_xpath('cats')['value']],\n",
    "            'posted_date':[e.text for e in sda.find_xpath('posted_date')['value']],\n",
    "        }\n",
    "        df_temp = pd.DataFrame(d)\n",
    "        if df is None:\n",
    "            df = df_temp.copy()\n",
    "        else:\n",
    "            df = df.append(df_temp,ignore_index=True)\n",
    "            df.index = list(range(len(df)))\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "# df = df.rename(columns={'loc':'location'})\n",
    "# df = df.rename(columns={'href':'job_url'})\n",
    "df['job_num'] = df.job_url.apply(lambda s:re.findall('job\\/[0-9]{5,12}\\/',s)[0].replace('/','').replace('job',''))\n",
    "df_black_rock_urls = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_shim(passed_in_driver, object):\n",
    "    x = object.location['x']\n",
    "    y = object.location['y']\n",
    "    scroll_by_coord = 'window.scrollTo(%s,%s);' % (\n",
    "        x,\n",
    "        y\n",
    "    )\n",
    "    scroll_nav_out_of_way = 'window.scrollBy(0, -120);'\n",
    "    passed_in_driver.execute_script(scroll_by_coord)\n",
    "    passed_in_driver.execute_script(scroll_nav_out_of_way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_read_more(ind):\n",
    "    driver = sda.sac.driver\n",
    "    sda.sac.goto(df.loc[ind].job_url)\n",
    "    source_element = sda.find_xpath('read_more')['value'][0]\n",
    "    if 'firefox' in driver.capabilities['browserName']:\n",
    "        scroll_shim(driver, source_element)\n",
    "    # scroll_shim is just scrolling it into view, you still need to hover over it to click using an action chain.\n",
    "    actions = sc.webdriver.ActionChains(driver)\n",
    "    actions.move_to_element(source_element)\n",
    "    actions.click()\n",
    "    actions.perform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "reqs = []\n",
    "inds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_start = 0\n",
    "for ind in [k for k in df.index if k>index_start]: \n",
    "    req = 0\n",
    "    description = None\n",
    "    try:\n",
    "        click_read_more(ind)\n",
    "        time.sleep(1)\n",
    "        description_element_list = sda.find_xpath('full_description')['value']\n",
    "        description = description_element_list[1].text\n",
    "        req = sda.find_xpath('req')['value'][0].text\n",
    "    except Exception as e2:\n",
    "        traceback.print_exc()    \n",
    "    sda.logger.info(f'finished index {ind}. req = {req}')\n",
    "                \n",
    "    descriptions.append(description)\n",
    "    reqs.append(req)\n",
    "    inds.append(ind)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blackrock_descriptions = pd.DataFrame({'req':reqs,'description':descriptions},index=inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blackrock = df_black_rock_urls.join(df_blackrock_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blackrock.iloc[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://boards.greenhouse.io/aqr'\n",
    "ss = sc.SelScrape(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.goto(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[2].get_attribute('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = ss.findxpath('//div[@class=\"opening\"]/a')['value']\n",
    "locs = ss.findxpath('//div[@class=\"opening\"]/span[@class=\"location\"]')['value']\n",
    "d = {\n",
    "    'title':[j.text for j in jobs ],\n",
    "    'href':[j.get_attribute('href') for j in jobs ],\n",
    "    'location':[j.text for j in locs],\n",
    "}\n",
    "df_aqr = pd.DataFrame(d)\n",
    "df_aqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.goto(df_aqr.iloc[2].href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 'contains(text(),\"What\") and contains(text(),\"Bring\")'\n",
    "# req_elements = ss.findxpath(f'//div[@id=\"content\"]//strong[{c}]/parent::p/following-sibling::ul/li')['value']\n",
    "#[e.text for e in req_elements]\n",
    "ss.findxpath(f'//div[@id=\"content\"]//strong[{c}]/parent::p/following-sibling::ul/li')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ss.findxpath('//div[@id=\"content\"]')['value'][0].text.split('\\n')\n",
    "lines_dict = {'team':lines[1],'role':lines[3],'experience':lines[5:]}\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 'contains(text(),\"What\") and contains(text(),\"Bring\")'\n",
    "descriptions = []\n",
    "for i in range(len(df_aqr)):\n",
    "    r = df_aqr.iloc[i]\n",
    "    ss.goto(r.href)\n",
    "    lines = ss.findxpath('//div[@id=\"content\"]')['value'][0].get_property('innerHTML')#.text.split('\\n')\n",
    "#     req_elements = ss.findxpath(f'//div[@id=\"content\"]//strong[{c}]/parent::p/following-sibling::ul/li')['value']\n",
    "#     experiences.append([e.text for e in req_elements])\n",
    "#     experiences.append(lines[5:])\n",
    "    descriptions.append(lines)\n",
    "df_aqr_2 = df_aqr.copy()\n",
    "# df_aqr_2['team'] = teams\n",
    "# df_aqr_2['role'] = roles\n",
    "df_aqr_2['description'] = descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss.goto(df_aqr.iloc[1].href)\n",
    "l = ss.findxpath('//div[@id=\"content\"]')['value'][0].get_property('innerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aqr_2_greenwich.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_aqr_2_greenwich = df_aqr_2[df_aqr_2.location.str.contains('Greenwich')]\n",
    "for i in range(len(df_aqr_2_greenwich)):\n",
    "    r = df_aqr_2_greenwich.iloc[i]\n",
    "    print(f'**************** {r.location} {r.title} ********************')\n",
    "    display(HTML(r.description))\n",
    "#     for d in r.description:\n",
    "#         print(d)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sc.SelScrape(headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verizon = None\n",
    "for i in range(1,200):\n",
    "    print(f'processing page {i}')\n",
    "    try:\n",
    "        main_url = f\"https://www.verizon.com/about/work/search/jobs?page={i}&per_page=100&sort_by=cfml10%2Cdesc\"\n",
    "        ss.goto(main_url)\n",
    "        h = ss.findxpath('//table')['value'][0].get_attribute('innerHTML').replace('\\n',' ')\n",
    "        df_temp = pd.read_html('<table>' + h + '</table>')[0]\n",
    "        urls = [e.get_attribute('href') for e in ss.findxpath('//td[@class=\"jobs_table_item_title\"]/a')['value']]\n",
    "        df_temp['url'] = urls\n",
    "        if df_verizon is None: \n",
    "            df_verizon = df_temp.copy()\n",
    "        else:\n",
    "            df_verizon = df_verizon.append(df_temp,ignore_index=True)\n",
    "            df_verizon.index = list(range(len(df_verizon)))\n",
    "    except Exception as e:\n",
    "        print(f'{str(e)}')\n",
    "        break\n",
    "df_verizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subset of tech from specific states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = ['FL','NJ','NY','CT']\n",
    "loc_list = [s for s in df_verizon.Location.unique() if any(sub in s for sub in state_list)]\n",
    "df_verizon_2 = df_verizon[(df_verizon.Location.isin(loc_list))]\n",
    "df_verizon_2_tech = df_verizon_2[df_verizon_2.Category.str.lower().str.contains('technology')]\n",
    "df_verizon_2_tech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_text = '//div[@class=\"cs_item_text\"]'\n",
    "def get_page(u):\n",
    "    ss.goto(u)\n",
    "    try:\n",
    "        r = ss.findxpath(main_text)\n",
    "        if r['status'] is not None:\n",
    "            return r['status']\n",
    "        h = r['value'][0].get_property('innerHTML')\n",
    "        return (h)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def get_html_list(df):\n",
    "    html_list = []\n",
    "    for i in range(len(df)):\n",
    "        r = df.iloc[i]\n",
    "        print(f'****** page {i}: {r.Title} {r.Location} ******')\n",
    "        try:        \n",
    "            p = get_page(r.url)\n",
    "            html_list.append(p)            \n",
    "            display(HTML(p))\n",
    "            print('')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            html_list.append(str(e))\n",
    "    return html_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "html_list = []\n",
    "for i in range(len(df_verizon_2_tech)):\n",
    "    r = df_verizon_2_tech.iloc[i]\n",
    "    try:        \n",
    "        p = get_page(r.url)\n",
    "        html_list.append(p)\n",
    "        print(f'******************* page {i}: {r.Title} {r.Location} ***********************')\n",
    "        display(HTML(p))\n",
    "        print('')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "df_verizon_2_tech['description'] = html_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verizon_2_tech_java = df_verizon_2_tech[df_verizon_2_tech.Title.str.lower().str.contains('java')]\n",
    "for i in range(len(df_verizon_2_tech_java)):\n",
    "    r = df_verizon_2_tech_java.iloc[i]\n",
    "    p = r.description\n",
    "    html_list.append(p)\n",
    "    print(f'******** page {i}: {r.Title} {r.Location} ***********')\n",
    "    display(HTML(p))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_verizon_python = df_verizon[df_verizon.Title.str.lower().str.contains('python')]\n",
    "df_verizon_python['description'] = get_html_list(df_verizon_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sc.SelScrape(headless=False)\n",
    "main_url = \"https://www.holdingschannel.com/all/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.goto(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_z = ss.findxpath('//a[contains(@href,\"letter=\")]')['value']\n",
    "a_to_z_hrefs = sorted(list(set([e.get_attribute('href') for e in a_to_z])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_list_xpath = '//h1[contains(text(),\"Alphabetical\")]/following-sibling::a[contains(@href,\"www.holdingschannel.com\")]'\n",
    "all_funds_hrefs = []\n",
    "all_funds_aums_this_year = []\n",
    "all_funds_aums_last_year = []\n",
    "for a_z_h in a_to_z_hrefs[:2]:\n",
    "    print(a_z_h)\n",
    "    ss.goto(a_z_h)\n",
    "    fund_element_list = ss.findxpath(fund_list_xpath)['value']\n",
    "    fund_href_list = [e.get_attribute('href') for e in fund_element_list]\n",
    "    for fund_href in fund_href_list[:2]: \n",
    "        try:\n",
    "            all_funds_hrefs.append(fund_href)\n",
    "            ss.goto(fund_href)        \n",
    "    #         [e.get_property('innerHTML') for e in ss.findxpath('//')['value']]\n",
    "            v = ss.findxpath('//td[@class=\"menuoff\"]/parent::tr/following-sibling::tr/td')['value'][0].get_property('innerHTML')\n",
    "            aum_list = re.findall(pattern='At[ ]{1,3}[0-1][0-9]/[[0-3][0-9]/20[1-3][0-9][:].{1,4}[0-9,]{1,}',string=v)\n",
    "            aum_values = [re.findall(pattern='\\$[0-9,]{1,}$',string=a)[0].replace('$','').replace(',','') for a in aum_list]\n",
    "            aum_values = [float(str(s)) for s in aum_values]\n",
    "            all_funds_aums_this_year.append(aum_values[0])\n",
    "            all_funds_aums_last_year.append(aum_values[1])\n",
    "        except Exception as e:\n",
    "            print(f'Exception on {fund_href}')\n",
    "            print(f'Exception:{str(e)}')\n",
    "        \n",
    "d = {'href':all_funds_hrefs,'aum_this_year':all_funds_aums_this_year,'aum_last_year':all_funds_aums_last_year}\n",
    "df_all_funds = pd.DataFrame(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_z_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_z_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = HOME_FOLDER + '/downloads/IA_FIRM_SEC_Feed_05_17_2019.xml'\n",
    "tree = ET.ElementTree(file=fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee =tree.findall('.//Firm/MainAddr')[0]\n",
    "ee.attrib['Strt1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = tree.findall('.//Firm')\n",
    "legal_names = [e.attrib['LegalNm'] for e in tree.findall('.//Firm/Info')]\n",
    "web_sites = [e.findall('.//WebAddrs/WebAddr')[0].text if len(e.findall('.//WebAddrs/WebAddr'))>0 else None for e in firms]\n",
    "def get_address_from_firm(e):\n",
    "    s1 =  '' if 'Strt1' not in e.attrib else e.attrib['Strt1']\n",
    "    s2 = '' if 'Strt2' not in e.attrib else e.attrib['Strt2']\n",
    "    city = '' if 'City' not in e.attrib else e.attrib['City']\n",
    "    state = '' if 'State' not in e.attrib else e.attrib['State']\n",
    "    return ' '.join([s1,s2,city,state])\n",
    "addresses = [get_address_from_firm(e) for e in tree.findall('.//Firm/MainAddr')]\n",
    "aums = [-1 if 'Q5DF3' not in e.attrib else int(e.attrib['Q5DF3']) for e in tree.findall('.//Firm/FormInfo/Part1A/Item5D')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_funds = pd.DataFrame({'legal_name':legal_names,'web_site':web_sites,'addresse':addresses,'aum':aums})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_funds = df_funds[(df_funds.aum>100) & (df_funds.aum<100000000)]\n",
    "df_small_funds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_small_funds_https = df_small_funds[['https' in str(s) for s in df_small_funds.web_site.str.lower()]]\n",
    "df_small_funds_https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = sc.SelScrape(headless=False)\n",
    "ss.goto(df_small_funds_https.iloc[0].web_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_elements = []\n",
    "hrefs = []\n",
    "for s in df_small_funds_https.web_site:\n",
    "    try:\n",
    "        ss.goto(s)\n",
    "        c = ss.findxpath('//*[contains(translate(text(),\"CAREER\",\"career\"),\"career\")]')\n",
    "        if c['value'] is not None:\n",
    "            print(s)\n",
    "            career_elements.append(c['value'][0])\n",
    "            hrefs.append(s)\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_small_funds_https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://medium.com/@InvestorDeck/hacking-investment-research-the-best-free-sites-to-research-stocks-37ff814adf68'\n",
    "ss  = sc.SelScrape(headless=False)\n",
    "ss.goto(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = '//p[contains(@class,\"raf-after--h3\")]/a'\n",
    "[e.text for e in ss.findxpath(h)['value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chase careers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "if os.path.abspath('.') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "if os.path.abspath('..') not in sys.path:\n",
    "    sys.path.append(os.path.abspath('..'))\n",
    "from selscrape import chase_careers as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    chase = cc.Chase()\n",
    "    df = chase.get_jobs()\n",
    "    df.to_csv('./temp_folder/chase_jobs.csv',index=False, encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chase = pd.read_csv('temp_folder/chase_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containsit(df,value_list,ignore_list=None):\n",
    "    df_current = df.copy()\n",
    "    for value in value_list:\n",
    "        df_current = df_current[df_current.description.str.lower().str.contains(value)]\n",
    "        if len(df_current)<1:\n",
    "            break\n",
    "    il = [] if ignore_list is None else ignore_list\n",
    "    for value in il:\n",
    "        df_current = df_current[~df_current.description.str.lower().str.contains(value)]\n",
    "        if len(df_current)<1:\n",
    "            break\n",
    "    return df_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chase_python = containsit(df_chase,'associate python'.split(),ignore_list=['ph.d','phd'])\n",
    "# df_chase_python = containsit(df_chase,'ph.d python'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_chase_python),df_chase_python.req.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for req in df_chase_python.req.unique():\n",
    "    dft = df_chase_python[df_chase_python.req==req]\n",
    "    print(dft.iloc[0].title)\n",
    "    print(dft.iloc[0].description)\n",
    "    print(\"*******************************************************\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridgewater Associates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge = pd.read_csv('temp_folder/bridgewater_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc in df_bridge[df_bridge.description.str.lower().str.contains(\"python\")].description:\n",
    "    print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
